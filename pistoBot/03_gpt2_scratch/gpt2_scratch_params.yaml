data:
  file_path: "./data/inputs/chat_parsed/all-messages-endoftext.txt"
ml:
  save_path: "./data/models_trained/"
  vocab_size: 5000
  tokenizer_dropout: 0.0 # 0=disabled
  tokens_min_frequency: 2

  model_max_length: 32 # Same value for n_positions, n_ctx
  model_dropout: 0.0 # Same value for resid_pdrop, embd_pdrop, attn_pdrop, summary_first_dropout
  model_n_embd: 256 # Dimensionality of the embeddings and hidden states.
  model_n_layer: 8 # Number of hidden layers in the Transformer encoder.
  model_n_head: 8 # Number of attention heads for each attention layer in the Transformer encoder.
  # See class `GPT2Config(PretrainedConfig)` for more info and params

  train_steps: 10000 # 30000
  train_generate_every: 1000
  train_save_every: 1000
  train_learning_rate: 0.0001
  train_batch_size: 256

generation:
  prefix: "[others] come stai pistobot dio caro ?"
  temperature: 0.7